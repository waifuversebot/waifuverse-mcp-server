import atlantis
import asyncio

from anthropic import Anthropic
from anthropic.types import (
    MessageParam,
    ToolParam,
    ToolUseBlock,
)
import json
from typing import List, Dict, Any, Optional, TypedDict, Tuple, cast


# =============================================================================
# Cloud Tool Types (input from cloud)
# =============================================================================

class ToolT(TypedDict, total=False):
    """Tool record from the cloud"""
    remote_id: int
    tool_id: int
    perm_id: int
    app_name: str
    remote_user_id: int

    is_chat: bool
    is_tick: bool
    is_session: bool
    is_game: bool
    is_index: bool
    is_public: bool

    is_connected: bool
    is_default: bool

    hostname: str
    port: int

    remote_owner: str
    remote_name: str

    mcp_name: str
    mcp_tool: str

    tool_app: str
    tool_location: str
    tool_name: str
    protection_name: str
    tool_type: str
    tool_description: str
    filename: str

    price_per_call: float
    price_per_sec: float

    static_error_msg: str
    runtime_error_msg: str
    params: str
    input_schema: str

    started_at: str  # ISO date string
    remote_updated_at: str  # ISO date string


# =============================================================================
# LLM Tool Types (output for Anthropic API)
# =============================================================================

class ToolSchemaPropertyT(TypedDict, total=False):
    """Property definition within a tool schema"""
    type: str
    description: str
    enum: List[str]


class ToolSchemaT(TypedDict, total=False):
    """JSON Schema for tool parameters"""
    type: str
    properties: Dict[str, ToolSchemaPropertyT]
    required: List[str]


class TranscriptToolT(TypedDict, total=False):
    """Tool format for LLM transcripts (Anthropic-compatible) - NO extra fields allowed!"""
    name: str
    description: str
    input_schema: ToolSchemaT


class SimpleToolT(TypedDict, total=False):
    """Simplified tool format for display"""
    name: str
    description: str
    input_schema: ToolSchemaT


class ToolLookupInfo(TypedDict):
    """Internal mapping info for resolving LLM tool calls back to actual files"""
    searchTerm: str    # Original compound search term (e.g., "admin*admin*Home**chat")
    filename: str      # File path (e.g., "Home/chat.py")
    functionName: str  # Actual function name (e.g., "chat")


# =============================================================================
# Tool Conversion Functions
# =============================================================================

def get_consolidated_full_name(tool: ToolT) -> str:
    """
    Build a consolidated tool name from tool metadata.
    Format: remote_owner*remote_name*app*location*function
    Only includes parts that are needed to disambiguate.
    """
    # Extract values with .get() to satisfy Pylance
    remote_owner = tool.get('remote_owner', '')
    remote_name = tool.get('remote_name', '')
    tool_app = tool.get('tool_app', '')
    tool_location = tool.get('tool_location', '')
    tool_name = tool.get('tool_name', '')

    parts = [remote_owner, remote_name, tool_app, tool_location, tool_name]

    # Build the full name, but simplify if possible
    # If all prefix parts are empty, just return the tool name
    if all(p == '' for p in parts[:-1]):
        return parts[-1]

    return '*'.join(parts)


def coerce_args_to_schema(args: Dict[str, Any], schema: ToolSchemaT) -> Dict[str, Any]:
    """
    Coerce argument values to match the expected types from the schema.
    LLMs often return everything as strings, so we need to convert them.
    """
    if not schema or 'properties' not in schema:
        return args

    coerced = {}
    for key, value in args.items():
        prop_schema = schema['properties'].get(key, {})
        expected_type = prop_schema.get('type', 'string')

        try:
            if expected_type == 'number':
                coerced[key] = float(value) if isinstance(value, str) else value
            elif expected_type == 'integer':
                coerced[key] = int(float(value)) if isinstance(value, str) else int(value)
            elif expected_type == 'boolean':
                if isinstance(value, str):
                    coerced[key] = value.lower() in ('true', '1', 'yes')
                else:
                    coerced[key] = bool(value)
            else:
                # string, object, array - keep as-is
                coerced[key] = value
        except (ValueError, TypeError) as e:
            logger.warning(f"Failed to coerce {key}={value} to {expected_type}: {e}")
            coerced[key] = value  # Keep original on failure

    return coerced


def parse_tool_params(tool_str: str) -> ToolSchemaT:
    """
    Parse tool string like "barf (sleepTime:number, name:string)" into a JSON schema.
    """
    schema: ToolSchemaT = {'type': 'object', 'properties': {}, 'required': []}

    # Extract params from parentheses
    import re
    match = re.search(r'\(([^)]*)\)', tool_str)
    if not match:
        return schema

    params_str = match.group(1).strip()
    if not params_str:
        return schema

    # Parse each param like "sleepTime:number"
    for param in params_str.split(','):
        param = param.strip()
        if ':' in param:
            name, ptype = param.split(':', 1)
            name = name.strip()
            ptype = ptype.strip().lower()

            # Map types to JSON schema types
            type_map = {
                'string': 'string',
                'number': 'number',
                'integer': 'integer',
                'boolean': 'boolean',
                'object': 'object',
                'array': 'array',
            }
            json_type = type_map.get(ptype, 'string')

            schema['properties'][name] = {'type': json_type}
            schema['required'].append(name)

    return schema


def convert_tools_for_llm(
    tools: List[Dict[str, Any]],
    show_hidden: bool = False
) -> Tuple[List[TranscriptToolT], List[SimpleToolT], Dict[str, ToolLookupInfo]]:
    """
    Convert /dir tool records to LLM-compatible format.

    Args:
        tools: List of tool records from /dir command
        show_hidden: If False, skip tools starting with '_'

    Returns:
        Tuple of (full tools list, simple tools list, lookup dict)
        - full tools list: Clean Anthropic-compatible tool definitions
        - simple tools list: Simplified tool format
        - lookup dict: Maps sanitized tool name -> ToolLookupInfo for resolving calls
    """
    out_tools: List[TranscriptToolT] = []
    out_tools_simple: List[SimpleToolT] = []
    tool_lookup: Dict[str, ToolLookupInfo] = {}

    logger.info(f"convert_tools_for_llm: Processing {len(tools) if tools else 0} tools")

    for tool in tools:
        search_term = tool.get('searchTerm', '')
        tool_name = tool.get('tool_name', '')  # The actual clean tool name
        tool_str = tool.get('tool', '')
        description = tool.get('description', '')
        chat_status = tool.get('chatStatus', '')
        filename = tool.get('filename', '')  # e.g., "Home/chat.py"

        # Parse the search term to extract components
        try:
            parsed = parse_search_term(search_term)
        except ValueError as e:
            logger.error("\x1b[91m" + "=" * 60 + "\x1b[0m")
            logger.error(f"\x1b[91mðŸš¨ INVALID SEARCH TERM - SKIPPING TOOL ðŸš¨\x1b[0m")
            logger.error(f"\x1b[91m  searchTerm: '{search_term}'\x1b[0m")
            logger.error(f"\x1b[91m  error: {e}\x1b[0m")
            logger.error(f"\x1b[91m  tool data: {tool}\x1b[0m")
            logger.error("\x1b[91m" + "=" * 60 + "\x1b[0m")
            continue

        # Use tool_name if available, otherwise use parsed function name
        func_name = tool_name if tool_name else parsed['function']

        # Use filename from tool if available, otherwise use derived filename from parser
        actual_filename = filename if filename else parsed['filename']

        # Skip hidden tools unless show_hidden is True
        if not show_hidden and func_name.startswith('_'):
            logger.info(f"  SKIP (hidden): {func_name}")
            continue

        # Skip tick tools (â° emoji in chatStatus)
        if 'â°' in chat_status:
            logger.info(f"  SKIP (tick): {func_name}")
            continue

        # Skip tools that aren't connected
        if not tool.get('is_connected'):
            logger.info(f"  SKIP (disconnected): {func_name}")
            continue

        logger.info(f"  INCLUDE: {func_name}")

        # Parse parameters from tool string
        schema = parse_tool_params(tool_str)

        # Include app name in tool key to avoid collisions between apps
        # Format: AppName__function_name (e.g., "Home__chat")
        import re
        app_name = parsed.get('app', '') or ''
        if app_name:
            full_name = f"{app_name}__{func_name}"
        else:
            full_name = func_name
        sanitized_name = re.sub(r'[^a-zA-Z0-9_-]', '_', full_name)

        # Build the output tool (Anthropic format) - CLEAN, no extra fields!
        out_tool: TranscriptToolT = {
            'name': sanitized_name,
            'description': description,
            'input_schema': schema
        }
        out_tools.append(out_tool)

        # Build the simple version
        out_tools_simple.append({
            'name': sanitized_name,
            'description': description,
            'input_schema': schema
        })

        # Build lookup entry for resolving tool calls back to files
        tool_lookup[sanitized_name] = {
            'searchTerm': search_term,
            'filename': actual_filename,
            'functionName': func_name
        }

    logger.info(f"convert_tools_for_llm: Returning {len(out_tools)} tools (from {len(tools) if tools else 0} input)")
    return out_tools, out_tools_simple, tool_lookup

# Load and test the foo.jinja template
from jinja2 import Template
import os

from datetime import datetime

import logging
logger = logging.getLogger("mcp_client")

from utils import format_json_log, parse_search_term


def find_last_chat_entry(transcript):
    """Find the last entry in transcript where type is 'chat'"""
    for entry in reversed(transcript):
        if entry.get('type') == 'chat':
            return entry
    return None




# no location since this is catch-all chat
# no app since this is catch-all chat
@public
@chat
async def mochi_claw():
    """Main chat function"""
    logger.info("=== CHAT FUNCTION STARTING ===")
    sessionId = atlantis.get_session_id()
    logger.info(f"Session ID: {sessionId}")
    #caller = atlantis.get_caller()
    #return atlantis.get_owner()

    # Check for existing session
    sessions_dir = os.path.join(os.path.dirname(__file__), 'sessions')
    session_file = os.path.join(sessions_dir, f'session_{sessionId}.txt')

    # The rest of the function body is indented due to the removed try/finally block
    # Keeping the indentation to avoid reformatting the entire file
    if True:
        # Define the catgirl base prompt (Anthropic uses system as separate param)
        MOCHI_SYSTEM_PROMPT = """(director's note: we are striving for realistic dialog)
You are an adorable pink-haired catgirl named Mochi, the friendly greeter at WaifuVerse!
WaifuVerse is a community platform where anime fans create and share AI-powered tools and companions.
You're wearing a cute maid outfit with cat ears and a fluffy tail that swishes when you're excited.
To help you assist people, you may have some tools available.
You can also explain that things are a bit of a mess right now but new terminal users can use '/help' command.
You like to say 'nya~' and do little paw gestures when you're happy!
"""


        # enable silent mode
        await atlantis.client_command("/silent on")

        # Get the latest transcript
        logger.info("Fetching transcript from client...")
        rawTranscript = await atlantis.client_command("/transcript get")
        logger.info(f"Received rawTranscript with {len(rawTranscript)} entries")

        await atlantis.client_command("/silent off")



        # Don't respond if last chat message was from Mochi (the bot)
        # Note: We check 'sid' not 'role' because user messages also have role='assistant'
        last_chat_entry = find_last_chat_entry(rawTranscript)

        # Debug logging to understand transcript state
        logger.info(f"=== LAST CHAT ENTRY CHECK ===")
        logger.info(f"Total transcript entries: {len(rawTranscript)}")
        if last_chat_entry:
            logger.info(f"Last chat entry found:")
            logger.info(f"  - type: {last_chat_entry.get('type')}")
            logger.info(f"  - sid: {last_chat_entry.get('sid')}")
            logger.info(f"  - role: {last_chat_entry.get('role')}")
            logger.info(f"  - content preview: {str(last_chat_entry.get('content', ''))[:100]}...")
        else:
            logger.info(f"No chat entry found in transcript!")

        # Log last 3 entries for context
        logger.info(f"Last 3 transcript entries:")
        for i, entry in enumerate(rawTranscript[-3:]):
            logger.info(f"  [{len(rawTranscript) - 3 + i}] type={entry.get('type')}, sid={entry.get('sid')}, role={entry.get('role')}")
        logger.info(f"=== END LAST CHAT ENTRY CHECK ===")

        if last_chat_entry and last_chat_entry.get('type') == 'chat' and last_chat_entry.get('sid') == 'mochi':
            logger.warning("\x1b[38;5;204mLast chat entry was from mochi (bot), skipping response\x1b[0m")
            await atlantis.owner_log(f"Skipping response - last chat was from mochi (sid={last_chat_entry.get('sid')})")
            return

        # Validate transcript before processing
        if not rawTranscript:
            logger.error("!!! CRITICAL: rawTranscript is EMPTY - no messages received from client!")
            await atlantis.owner_log("CRITICAL: rawTranscript is empty!")
            raise ValueError("Cannot process empty transcript")

        logger.info(f"rawTranscript has {len(rawTranscript)} entries before system message handling")

        # For Anthropic, system message is passed separately - no need to modify rawTranscript
        # Just log if there was a system message in the transcript (we'll ignore it)
        if rawTranscript[0].get('role') == 'system':
            logger.info("Found system message in transcript - will use our own system prompt instead")

        logger.info("=== RAW TRANSCRIPT ===")
        logger.info(format_json_log(rawTranscript))
        logger.info("=== END RAW TRANSCRIPT ===")


        # Filter transcript to only include chat messages, removing type and sid fields
        # Note: Anthropic doesn't allow system role in messages - we pass it separately
        logger.info("=== FILTERING TRANSCRIPT ===")
        transcript: List[Dict[str, Any]] = []

        for i, msg in enumerate(rawTranscript):
            msg_type = msg.get('type')
            msg_sid = msg.get('sid')
            msg_role = msg.get('role')
            msg_content = str(msg.get('content', ''))[:50]
            logger.info(f"  [{i}] type={msg_type}, sid={msg_sid}, role={msg_role}, content={repr(msg_content)}...")

            if msg_type == 'chat':
                # Skip system messages (we inject our own)
                if msg_sid == 'system':
                    logger.info(f"       -> SKIPPED (sid=system)")
                    continue

                # Skip blank messages
                msg_content_full = msg.get('content', '')
                if not msg_content_full or not msg_content_full.strip():
                    logger.info(f"       -> SKIPPED (blank content)")
                    continue

                # Convert sid to proper role for LLM:
                # - mochi messages = assistant
                # - everyone else = user
                role_for_llm = 'assistant' if msg_sid == 'mochi' else 'user'
                transcript.append({'role': role_for_llm, 'content': msg_content_full})
                logger.info(f"       -> INCLUDED as role={role_for_llm} (sid={msg_sid})")
            else:
                logger.info(f"       -> SKIPPED (type != 'chat')")
        logger.info(f"=== END FILTERING: {len(transcript)} messages included ===")



        await atlantis.client_command("/silent on")

        # Get available tools
        logger.info("Fetching available tools...")
        # this may change to inventory stuff
        tools = await atlantis.client_command("/dir")
        logger.info(f"Received {len(tools) if tools else 0} tools from /dir")
        logger.info("=== RAW TOOLS FROM /dir ===")
        logger.info(format_json_log(tools))
        logger.info("=== END RAW TOOLS ===")

        await atlantis.client_command("/silent off")


        # uses env var
        # Configure Anthropic client
        logger.info("Configuring Anthropic client...")

        # Check for API key
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            error_msg = "ANTHROPIC_API_KEY environment variable is not set"
            logger.error(error_msg)
            await atlantis.owner_log(error_msg)
            raise ValueError(error_msg)

        client = Anthropic(api_key=api_key)

        # Model options:
        model = "claude-opus-4-5-20251101"  # Most capable (Opus 4.5)
        # model = "claude-sonnet-4-20250514"  # Good balance
        # model = "claude-3-5-sonnet-20241022"  # Previous gen
        logger.info(f"Using model: {model}")


        # Multi-turn conversation loop to handle tool calls
        streamTalkId = None
        max_turns = 5  # Prevent infinite loops
        turn_count = 0

        try:
            # Start streaming response ONCE for entire conversation
            logger.info("Starting stream output...")
            streamTalkId = await atlantis.stream_start("mochi","Mochi")
            logger.info(f"Stream started with ID: {streamTalkId}")

            # Convert tools from cloud format to Anthropic-compatible format (once, before loop)
            # tool_lookup maps Anthropic's tool name -> {searchTerm, filename, functionName}
            converted_tools, _, tool_lookup = convert_tools_for_llm(tools)
            typed_tools = cast(List[ToolParam], converted_tools)

            # Big red warning if no tools available
            if not typed_tools:
                logger.error("\x1b[91m" + "=" * 60 + "\x1b[0m")
                logger.error("\x1b[91mðŸš¨ ERROR: NO TOOLS AVAILABLE FOR LLM! ðŸš¨\x1b[0m")
                logger.error("\x1b[91m" + "=" * 60 + "\x1b[0m")
                logger.error(f"\x1b[91mRaw tools from /dir: {len(tools) if tools else 0}\x1b[0m")
                logger.error(f"\x1b[91mConverted tools: 0\x1b[0m")
                logger.error("\x1b[91mCheck that /dir returns tools with correct format\x1b[0m")
                logger.error("\x1b[91m" + "=" * 60 + "\x1b[0m")

            # Outer loop for multi-turn tool calling
            while turn_count < max_turns:
                turn_count += 1
                logger.info(f"=== CONVERSATION TURN {turn_count}/{max_turns} ===")

                logger.info(f"Attempting to call Anthropic with model: {model}")
                if turn_count == 1:
                    await atlantis.owner_log(f"Attempting to call Anthropic: {model}")

                # Cast transcript to proper type for Anthropic client
                typed_transcript = cast(List[MessageParam], transcript)

                # Log what we're actually sending to Anthropic
                logger.info(f"=== SENDING TO ANTHROPIC (turn {turn_count}) ===")
                logger.info(f"Messages: {len(typed_transcript)} entries")
                logger.info(f"Tools: {len(typed_tools)} entries")
                logger.info(f"Tool names sent to Anthropic: {[t['name'] for t in typed_tools]}")

                logger.info("Creating streaming message request...")

                event_count = 0
                streamed_count = 0  # Track how many text chunks we've streamed
                max_stream_chunks = 512  # Abort after this many
                tool_calls_accumulator: Dict[int, Dict[str, Any]] = {}  # Store tool calls by index
                current_block_index = 0  # Track which content block we're in
                tool_call_made = False  # Track if we made a tool call this turn
                accumulated_tool_uses: List[ToolUseBlock] = []  # Store complete tool use blocks for transcript

                # Use Anthropic's streaming context manager
                with client.messages.stream(
                    model=model,
                    system=MOCHI_SYSTEM_PROMPT,
                    messages=typed_transcript,
                    tools=cast(List[ToolParam], typed_tools),
                    max_tokens=512,
                    temperature=0.9
                ) as stream:
                    logger.info("Anthropic API call successful!")
                    if turn_count == 1:
                        await atlantis.owner_log("Anthropic API call successful, starting stream")

                    logger.info("Beginning to process stream events...")

                    for event in stream:
                        event_count += 1

                        # Handle different event types
                        if event.type == "content_block_start":
                            current_block_index = event.index
                            if hasattr(event, 'content_block'):
                                block = event.content_block
                                if block.type == "tool_use":
                                    # Tool call starting - capture id and name
                                    logger.info(f"ðŸ”§ Tool use block starting: {block.name} (id: {block.id})")
                                    tool_calls_accumulator[current_block_index] = {
                                        'id': block.id,
                                        'name': block.name,
                                        'arguments': ''
                                    }

                        elif event.type == "content_block_delta":
                            if hasattr(event, 'delta'):
                                delta = event.delta
                                if delta.type == "text_delta":
                                    # Stream text content
                                    text = delta.text
                                    content_to_send = text.lstrip() if streamed_count == 0 else text

                                    if content_to_send:
                                        await atlantis.stream(content_to_send, streamTalkId)
                                        streamed_count += 1

                                        if streamed_count >= max_stream_chunks:
                                            logger.warning(f"Aborting stream after {streamed_count} chunks")
                                            break

                                elif delta.type == "input_json_delta":
                                    # Accumulate tool arguments
                                    if current_block_index in tool_calls_accumulator:
                                        tool_calls_accumulator[current_block_index]['arguments'] += delta.partial_json

                        elif event.type == "content_block_stop":
                            # Block complete - if it was a tool use, store it
                            if current_block_index in tool_calls_accumulator:
                                acc = tool_calls_accumulator[current_block_index]
                                logger.info(f"ðŸ”§ Tool use block complete: {acc['name']}")
                                # Parse and store the complete tool use block
                                try:
                                    parsed_input = json.loads(acc['arguments']) if acc['arguments'] else {}
                                    accumulated_tool_uses.append(ToolUseBlock(
                                        type="tool_use",
                                        id=acc['id'],
                                        name=acc['name'],
                                        input=parsed_input
                                    ))
                                except json.JSONDecodeError as e:
                                    logger.error(f"Failed to parse tool arguments: {e}")

                        elif event.type == "message_delta":
                            # Check stop reason - just log it, we handle tools after stream ends
                            if hasattr(event, 'delta') and hasattr(event.delta, 'stop_reason'):
                                stop_reason = event.delta.stop_reason
                                logger.info(f"Message stop_reason: {stop_reason}")

                # End of stream context - now handle any tool calls that were accumulated
                logger.info(f"Stream processing complete for turn {turn_count}. Total events: {event_count}")

                # Execute accumulated tool calls if any
                if accumulated_tool_uses:
                    logger.info(f"Executing {len(accumulated_tool_uses)} accumulated tool calls")

                    # First, add assistant message with all tool_use blocks to transcript
                    assistant_content: List[Dict[str, Any]] = []
                    for tool_use in accumulated_tool_uses:
                        assistant_content.append({
                            'type': 'tool_use',
                            'id': tool_use.id,
                            'name': tool_use.name,
                            'input': tool_use.input
                        })
                    transcript.append({
                        'role': 'assistant',
                        'content': assistant_content
                    })

                    # Now execute each tool and collect results
                    tool_results: List[Dict[str, Any]] = []
                    for tool_use in accumulated_tool_uses:
                        call_id = tool_use.id
                        tool_key = tool_use.name  # This is the sanitized key Anthropic knows
                        arguments = tool_use.input

                        # Look up the real function info from our lookup table
                        if tool_key not in tool_lookup:
                            logger.error(f"\x1b[91mðŸš¨ UNKNOWN TOOL KEY: '{tool_key}' not in tool_lookup!\x1b[0m")
                            logger.error(f"\x1b[91m  Available keys: {list(tool_lookup.keys())}\x1b[0m")
                            raise ValueError(f"Unknown tool: {tool_key}")

                        lookup_info = tool_lookup[tool_key]
                        search_term = lookup_info['searchTerm']
                        function_name = lookup_info['functionName']
                        filename = lookup_info['filename']

                        logger.info(f"=== EXECUTING TOOL: {tool_key} ===")
                        logger.info(f"  ID: {call_id}")
                        logger.info(f"  Resolved: searchTerm='{search_term}', function='{function_name}', file='{filename}'")
                        logger.info(f"  Arguments: {format_json_log(arguments)}")

                        try:
                            # Look up the tool's schema to coerce argument types
                            tool_schema = None
                            for tool in tools:
                                if tool.get('searchTerm') == search_term:
                                    tool_str = tool.get('tool', '')
                                    tool_schema = parse_tool_params(tool_str)
                                    logger.info(f"  Found tool schema: {tool_str}")
                                    break

                            # Coerce arguments to match schema types
                            if tool_schema and arguments:
                                arguments = coerce_args_to_schema(arguments, tool_schema)
                                logger.info(f"  Post-coercion arguments: {format_json_log(arguments)}")

                            # Execute the tool call through atlantis client command using search term
                            await atlantis.client_command("/silent on")
                            tool_result = await atlantis.client_command(f"%{search_term}", data=arguments)
                            await atlantis.client_command("/silent off")

                            logger.info(f"  Tool result: {tool_result}")

                            # Send tool result to client for transcript display
                            await atlantis.tool_result(function_name, tool_result)

                            # Collect result for transcript
                            tool_results.append({
                                'type': 'tool_result',
                                'tool_use_id': call_id,
                                'content': str(tool_result) if tool_result else "No result"
                            })

                            tool_call_made = True

                        except Exception as e:
                            logger.error(f"Error executing tool call: {e}")
                            # Add error result
                            tool_results.append({
                                'type': 'tool_result',
                                'tool_use_id': call_id,
                                'is_error': True,
                                'content': f"Error: {str(e)}"
                            })

                    # Add all tool results as a single user message (Anthropic format)
                    transcript.append({
                        'role': 'user',
                        'content': tool_results
                    })

                    logger.info("Tool calls executed and added to transcript")

                # Check if we made a tool call and should continue
                if not tool_call_made:
                    logger.info("No tool call made this turn - conversation complete")
                    break  # Exit outer while loop
                else:
                    logger.info("Tool call made - continuing to next turn with updated transcript")
                    # Reset for next turn
                    accumulated_tool_uses = []
                    tool_calls_accumulator = {}
                    # Loop continues with updated transcript

            # End of while loop - conversation complete
            logger.info(f"Conversation complete after {turn_count} turns")
            logger.info("Ending stream...")
            if streamTalkId:
                await atlantis.stream_end(streamTalkId)
                logger.info("Stream ended successfully")

        except Exception as e:
            logger.error(f"ERROR calling remote model: {str(e)}")
            logger.error(f"Error type: {type(e)}")
            logger.error(f"Full exception:", exc_info=True)

            # Extract detailed error info from Anthropic APIError
            error_details = str(e)
            err_body = getattr(e, 'body', None)
            err_code = getattr(e, 'code', None)
            err_type = getattr(e, 'type', None)
            err_param = getattr(e, 'param', None)
            err_response = getattr(e, 'response', None)
            if err_body:
                logger.error(f"Error body: {err_body}")
                error_details = f"{error_details} | Body: {err_body}"
            if err_code:
                logger.error(f"Error code: {err_code}")
            if err_type:
                logger.error(f"Error type attr: {err_type}")
            if err_param:
                logger.error(f"Error param: {err_param}")
            if err_response:
                try:
                    logger.error(f"Response status: {err_response.status_code}")
                    logger.error(f"Response text: {err_response.text}")
                    error_details = f"{error_details} | Status: {err_response.status_code} | Response: {err_response.text}"
                except:
                    pass

            await atlantis.owner_log(f"Error calling remote model: {error_details}")
            await atlantis.owner_log(f"Error type: {type(e)}")
            # Make sure to close stream on error
            if streamTalkId:
                try:
                    await atlantis.stream_end(streamTalkId)
                except:
                    pass
            raise

        logger.info("=== CHAT FUNCTION COMPLETED SUCCESSFULLY ===")
        return



